{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Analysis on Diabetes Progression\n",
    "\n",
    "**Author:** Evan Gks  \n",
    "**Date:** 2025-02-02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "This notebook demonstrates a full machine learning workflow to predict diabetes progression using scikit‑learn’s linear regression. The analysis includes dataset exploration, preprocessing, model training (using both the closed‑form solution and a custom gradient descent implementation), evaluation with multiple metrics, cross‑validation, and comparison with a baseline dummy model. Detailed visualizations and discussions of limitations and improvements are provided for production‑quality insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Statement\n",
    "\n",
    "Diabetes is a chronic disease with significant health implications. Early prediction of disease progression can enable proactive care and better management. In this notebook, we aim to predict a quantitative measure of disease progression (a continuous target variable) using the diabetes dataset from scikit‑learn. Our approach includes thorough exploratory analysis, feature engineering, and model comparison to provide a robust predictive solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Description and Exploratory Analysis\n",
    "\n",
    "The dataset used is the well‑known *diabetes dataset* provided in scikit‑learn. It contains 10 baseline variables (features) measured on 442 diabetes patients and a target variable representing a quantitative measure of disease progression one year after baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diabetes Dataset Feature Descriptions\n",
    "\n",
    "The diabetes dataset contains 10 baseline variables (features) collected from 442 patients, along with a target variable that represents a quantitative measure of disease progression one year after baseline. Below is a description of each feature:\n",
    "\n",
    "1. **age**: Age of the patient (normalized). Age can be an important factor, as older individuals may experience increased risk or severity of diabetes-related complications.\n",
    "\n",
    "2. **sex**: Sex (normalized). Captures the gender of the patient, which could influence certain risk factors or disease progression patterns.\n",
    "\n",
    "3. **bmi**: Body Mass Index (normalized). BMI is a widely used measure that accounts for weight relative to height, serving as a proxy for body fat. High BMI is strongly associated with diabetes risk and progression.\n",
    "\n",
    "4. **bp**: Average blood pressure (normalized). Blood pressure is a crucial factor as hypertension frequently co-occurs with diabetes and can exacerbate complications.\n",
    "\n",
    "5. **s1**: Serum TC (Total Cholesterol) level (normalized). Measures cholesterol levels in the blood, which may serve as an indicator of cardiovascular health, often associated with diabetes.\n",
    "\n",
    "6. **s2**: Serum LDL (Low-Density Lipoprotein) level (normalized). LDL cholesterol is often referred to as \"bad cholesterol,\" as elevated levels are linked to cardiovascular risk.\n",
    "\n",
    "7. **s3**: Serum HDL (High-Density Lipoprotein) level (normalized). HDL cholesterol, or \"good cholesterol,\" is thought to help clear cholesterol from the bloodstream, protecting against heart disease.\n",
    "\n",
    "8. **s4**: Serum triglycerides (TG) level (normalized). Triglycerides represent another type of fat in the blood; high triglyceride levels can increase the risk of diabetes complications.\n",
    "\n",
    "9. **s5**: Serum concentration of LTG (log-transformed measure of insulin sensitivity) (normalized). A measure particularly pertinent to diabetes, as it indicates the body's ability to use insulin effectively.\n",
    "\n",
    "10. **s6**: Blood glucose level (normalized). Blood glucose, or blood sugar levels, directly relate to diabetes management and progression.\n",
    "\n",
    "**Target variable:**\n",
    "- **disease_progression**: A quantitative measure of diabetes disease progression one year after baseline. This is a continuous value representing the progression of the condition in patients.\n",
    "\n",
    "All features in the dataset are normalized to have a mean of 0 and a standard deviation of 1, making them dimensionless and ensuring comparability during modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we load the data, examine its statistical properties, check for missing values, and visualize the data distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset from scikit-learn\n",
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for features and a Series for the target variable\n",
    "df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "df['disease_progression'] = diabetes.target\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Summary\n",
    "Below is the statistical summary of the dataset, which gives an overview of the distribution of each feature and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistical summary\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairplots and Distributions\n",
    "\n",
    "We create pairplots for a subset of features (due to the high number of variables) to visualize potential relationships and distributions. We also check for any missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions using histograms\n",
    "df.hist(bins=20, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pairplot for a few selected features and the target variable\n",
    "selected_features = ['bmi', 'bp', 's1', 'disease_progression']\n",
    "sns.pairplot(df[selected_features])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correlation Matrix\n",
    "\n",
    "Understanding the correlation between features can provide insights into multicollinearity and feature redundancy. A correlation matrix visualizes these relationships, with values ranging from -1 to 1.\n",
    "\n",
    "- **Positive correlation (close to 1):**  Indicates that as one feature increases, the other tends to increase as well.\n",
    "- **Negative correlation (close to -1):** Indicates that as one feature increases, the other tends to decrease.\n",
    "- **Correlation close to 0:** Indicates a weak or no linear relationship between the features.\n",
    "\n",
    "Analyzing the correlation matrix can help in feature selection and understanding the underlying structure of the data. We will now compute and visualize the correlation matrix for the diabetes dataset features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Visualize the correlation matrix using a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "The preprocessing pipeline includes:\n",
    "- **Feature Selection:** All 10 baseline variables are used.\n",
    "- **Outlier Handling:** For demonstration purposes, we assume the data is clean.\n",
    "- **Scaling:** We standardize the features.\n",
    "- **Train-Test Split:** We split the data into training and testing sets.\n",
    "\n",
    "Below, we implement these steps using scikit‑learn’s pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop('disease_progression', axis=1)\n",
    "y = df['disease_progression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that first scales the data then applies linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mathematical Explanation of Linear Regression\n",
    "\n",
    "The linear regression model assumes a linear relationship between the input features and the target variable:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p + \\epsilon$\n",
    "\n",
    "The goal is to estimate the coefficients $\\beta = (\\beta_0, \\beta_1, \\dots, \\beta_p)$ by minimizing the mean squared error (MSE):\n",
    "\n",
    "$J(\\beta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\hat{y}^{(i)} - y^{(i)} \\right)^2$\n",
    "\n",
    "Two common approaches to solve for $\\beta$ are:\n",
    "1. **Normal Equation (Closed-Form Solution):**\n",
    "   $\\beta = (X^T X)^{-1} X^T y$\n",
    "\n",
    "2. **Gradient Descent:**\n",
    "   Iteratively update:\n",
    "   $\\beta := \\beta - \\alpha \\frac{\\partial J(\\beta)}{\\partial \\beta}$\n",
    "   where $(\\alpha)$ is the learning rate.\n",
    "\n",
    "In this notebook, we demonstrate both approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Hyperparameter Tuning\n",
    "\n",
    "We train two linear regression models:\n",
    "- **Model 1:** Using scikit‑learn’s `LinearRegression` (which internally uses the closed‑form solution).\n",
    "- **Model 2:** A custom gradient descent implementation.\n",
    "\n",
    "We also tune the gradient descent hyperparameters (learning rate and number of iterations) and compare the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Model 1: Closed-form solution using scikit-learn\n",
    "# -------------------------\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_sklearn = pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Model 2: Gradient Descent Implementation\n",
    "# -------------------------\n",
    "\n",
    "# We implement gradient descent on the scaled features.\n",
    "# First, we scale the training and test data (using the same scaler from the pipeline)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Add an intercept term (column of ones)\n",
    "X_train_gd = np.hstack([np.ones((X_train_scaled.shape[0], 1)), X_train_scaled])\n",
    "X_test_gd = np.hstack([np.ones((X_test_scaled.shape[0], 1)), X_test_scaled])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, learning_rate=0.01, n_iterations=1000):\n",
    "    \"\"\"\n",
    "    Performs gradient descent for linear regression.\n",
    "    \n",
    "    Parameters:\n",
    "        X : np.array, shape (m, n+1) - features with intercept term added\n",
    "        y : np.array, shape (m,) - target variable\n",
    "        learning_rate : float - step size for gradient descent\n",
    "        n_iterations : int - number of iterations to run gradient descent\n",
    "        \n",
    "    Returns:\n",
    "        theta : np.array, shape (n+1,) - learned parameters\n",
    "        cost_history : list of cost values for each iteration\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    cost_history = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Predictions\n",
    "        y_pred = X.dot(theta)\n",
    "        # Compute error\n",
    "        error = y_pred - y\n",
    "        # Compute cost (MSE)\n",
    "        cost = (1/(2*m)) * np.sum(error**2)\n",
    "        cost_history.append(cost)\n",
    "        # Update parameters (gradient descent update rule)\n",
    "        theta = theta - (learning_rate/m) * X.T.dot(error)\n",
    "        \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gradient descent with tuned hyperparameters\n",
    "gd_learning_rate = 0.1\n",
    "gd_iterations = 1500\n",
    "\n",
    "theta_gd, cost_history = gradient_descent(X_train_gd, y_train.values, learning_rate=gd_learning_rate, n_iterations=gd_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data using the learned parameters\n",
    "y_pred_gd = X_test_gd.dot(theta_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cost history to inspect convergence\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost (MSE)\")\n",
    "plt.title(\"Gradient Descent Convergence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "We evaluate both models using the following metrics:\n",
    "- **Mean Absolute Error (MAE)**\n",
    "- **Mean Squared Error (MSE)**\n",
    "- **Root Mean Squared Error (RMSE)**\n",
    "- **R-squared (\\(R^2\\))**\n",
    "\n",
    "Below is the evaluation and a comparison table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Evaluation for scikit-learn Linear Regression\n",
    "mae_sklearn = mean_absolute_error(y_test, y_pred_sklearn)\n",
    "mse_sklearn = mean_squared_error(y_test, y_pred_sklearn)\n",
    "rmse_sklearn = np.sqrt(mse_sklearn)\n",
    "r2_sklearn = r2_score(y_test, y_pred_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for Gradient Descent Implementation\n",
    "mae_gd = mean_absolute_error(y_test, y_pred_gd)\n",
    "mse_gd = mean_squared_error(y_test, y_pred_gd)\n",
    "rmse_gd = np.sqrt(mse_gd)\n",
    "r2_gd = r2_score(y_test, y_pred_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to display the evaluation metrics\n",
    "metrics = pd.DataFrame({\n",
    "    \"Model\": [\"LinearRegression (Normal Equation)\", \"Gradient Descent\"],\n",
    "    \"MAE\": [mae_sklearn, mae_gd],\n",
    "    \"MSE\": [mse_sklearn, mse_gd],\n",
    "    \"RMSE\": [rmse_sklearn, rmse_gd],\n",
    "    \"R2\": [r2_sklearn, r2_gd]\n",
    "})\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- **MAE/MSE/RMSE:** Lower values indicate better performance.\n",
    "- **\\(R^2\\):** Values closer to 1.0 indicate a better fit.\n",
    "\n",
    "Both implementations provide similar results, validating our custom gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Residual Analysis and Visualization\n",
    "\n",
    "We now perform a residual analysis by plotting:\n",
    "- **Residuals vs. Predicted Values**\n",
    "- **Predicted vs. Actual Values**\n",
    "\n",
    "These visualizations help assess the model’s fit and check for any systematic errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plot for the scikit-learn model\n",
    "residuals = y_test - y_pred_sklearn\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y_pred_sklearn, residuals, alpha=0.7)\n",
    "plt.hlines(0, min(y_pred_sklearn), max(y_pred_sklearn), colors='red', linestyles='dashed')\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Analysis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Predicted vs. Actual values\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y_test, y_pred_sklearn, alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Predicted vs. Actual Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cross-Validation and Baseline Comparison\n",
    "\n",
    "### Cross-Validation\n",
    "\n",
    "We apply a 5‑fold cross‑validation strategy on the full pipeline to assess model stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Define 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(pipeline, X, y, cv=kf, scoring='r2')\n",
    "print(\"Cross-Validation R2 Scores:\", cv_scores)\n",
    "print(\"Mean CV R2 Score:\", np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Dummy Model\n",
    "\n",
    "We compare our model against a baseline dummy regressor that always predicts the mean of the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "# Initialize and evaluate DummyRegressor\n",
    "dummy = DummyRegressor(strategy=\"mean\")\n",
    "dummy_scores = cross_val_score(dummy, X, y, cv=kf, scoring='r2')\n",
    "print(\"Dummy Regressor CV R2 Scores:\", dummy_scores)\n",
    "print(\"Mean Dummy R2 Score:\", np.mean(dummy_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the coefficients from the trained linear model in the pipeline\n",
    "lr_model = pipeline.named_steps['lr']\n",
    "coefficients = lr_model.coef_\n",
    "features = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"Coefficient\": coefficients\n",
    "}).sort_values(by=\"Coefficient\", key=abs, ascending=False)\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=coef_df, x=\"Coefficient\", y=\"Feature\", palette=\"viridis\")\n",
    "plt.title(\"Feature Importance based on Coefficients\")\n",
    "plt.xlabel(\"Coefficient Value\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Discussion of Limitations and Potential Improvements\n",
    "\n",
    "### Limitations:\n",
    "- **Linear Assumption:** The linear model may oversimplify complex relationships.\n",
    "- **Outlier Sensitivity:** Although the dataset is pre‑processed, extreme values could affect the model.\n",
    "- **Gradient Descent Tuning:** Our custom gradient descent is rudimentary and may require further tuning (adaptive learning rates, momentum, etc.) for larger datasets.\n",
    "\n",
    "### Potential Improvements:\n",
    "- **Non‑linear Models:** Consider tree‑based models or kernel methods for capturing non‑linear patterns.\n",
    "- **Regularization:** Apply Ridge or Lasso regression to reduce overfitting.\n",
    "- **Feature Engineering:** Explore interaction terms or polynomial features.\n",
    "- **Advanced Optimization:** Use more sophisticated optimizers (e.g., Adam) for gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "In this notebook, we demonstrated a full machine learning workflow—from exploratory data analysis to model evaluation—using linear regression to predict diabetes progression. Both the closed‑form solution and a custom gradient descent implementation yielded comparable results. The comprehensive analysis, including cross‑validation and baseline comparison, provides confidence in the model’s predictive power. Future work may explore non‑linear approaches and regularization techniques to further enhance model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 12. References\n",
    "\n",
    "- Scikit‑learn documentation: [https://scikit-learn.org](https://scikit-learn.org)\n",
    "- Diabetes dataset description: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*.\n",
    "- Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*.\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook was developed as an educational resource to demonstrate a production‑quality machine learning pipeline.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
